{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worachit/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'limit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m data_loader_train, data_loader_val, data_loader_test\n\u001b[1;32m     33\u001b[0m \u001b[39m# dataset_val exists because i'm lazy\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m dataset_train, dataset_val, dataset_test \u001b[39m=\u001b[39m get_datasets(class_map)\n\u001b[1;32m     36\u001b[0m data_loader_train, _, data_loader_test \u001b[39m=\u001b[39m get_data_loaders(\n\u001b[1;32m     37\u001b[0m     dataset_train,\n\u001b[1;32m     38\u001b[0m     dataset_val,\n\u001b[1;32m     39\u001b[0m     dataset_test,\n\u001b[1;32m     40\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[39m# next(iter(data_loader_train))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m, in \u001b[0;36mget_datasets\u001b[0;34m(class_map)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_datasets\u001b[39m(class_map):\n\u001b[1;32m      9\u001b[0m     \u001b[39m# verify that class_map can be non-optional\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m class_map \u001b[39min\u001b[39;00m class_maps\u001b[39m.\u001b[39mclass_maps:\n\u001b[0;32m---> 11\u001b[0m         \u001b[39mreturn\u001b[39;00m lsb_datasets(class_map)\n\u001b[1;32m     12\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/panoptic-lsb/data/training_utils.py:176\u001b[0m, in \u001b[0;36mlsb_datasets\u001b[0;34m(class_map, dataset)\u001b[0m\n\u001b[1;32m    165\u001b[0m transform \u001b[39m=\u001b[39m {\n\u001b[1;32m    166\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mresize\u001b[39m\u001b[39m'\u001b[39m: [image_size, image_size],\n\u001b[1;32m    167\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mflip\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39m# 'elastic': None,\u001b[39;00m\n\u001b[1;32m    173\u001b[0m }\n\u001b[1;32m    175\u001b[0m \u001b[39m# get datasets\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m dataset_train \u001b[39m=\u001b[39m construct_dataset(\n\u001b[1;32m    177\u001b[0m     idxs\u001b[39m=\u001b[39;49mindices[:\u001b[39mint\u001b[39;49m(N \u001b[39m*\u001b[39;49m val_p)],\n\u001b[1;32m    178\u001b[0m     class_map\u001b[39m=\u001b[39;49mclass_map,\n\u001b[1;32m    179\u001b[0m     transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    180\u001b[0m     aug_mult\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    181\u001b[0m dataset_val \u001b[39m=\u001b[39m construct_dataset(\n\u001b[1;32m    182\u001b[0m     idxs\u001b[39m=\u001b[39mindices[\u001b[39mint\u001b[39m(N \u001b[39m*\u001b[39m val_p):\u001b[39mint\u001b[39m(N \u001b[39m*\u001b[39m test_p)],\n\u001b[1;32m    183\u001b[0m     class_map\u001b[39m=\u001b[39mclass_map,\n\u001b[1;32m    184\u001b[0m     transform\u001b[39m=\u001b[39mtransform)\n\u001b[1;32m    185\u001b[0m dataset_test \u001b[39m=\u001b[39m construct_dataset(\n\u001b[1;32m    186\u001b[0m     idxs\u001b[39m=\u001b[39mtest_indices,\n\u001b[1;32m    187\u001b[0m     class_map\u001b[39m=\u001b[39mclass_map,\n\u001b[1;32m    188\u001b[0m     transform\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mresize\u001b[39m\u001b[39m'\u001b[39m: [image_size, image_size]})\n",
      "File \u001b[0;32m~/panoptic-lsb/data/training_utils.py:49\u001b[0m, in \u001b[0;36mconstruct_dataset\u001b[0;34m(dataset, transform, idxs, bands, class_map, aug_mult, padding, images_path, ann_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m Dataset \u001b[39m=\u001b[39m datasets[dataset][\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     transform \u001b[39m=\u001b[39m get_transform(transform)\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m ann_path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mannotations\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m datasets[dataset]:\n",
      "File \u001b[0;32m~/panoptic-lsb/data/training_utils.py:81\u001b[0m, in \u001b[0;36mget_transform\u001b[0;34m(transforms)\u001b[0m\n\u001b[1;32m     79\u001b[0m         kwargs\u001b[39m.\u001b[39mupdate(args)\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m pos_args, kwargs\n\u001b[0;32m---> 81\u001b[0m transforms \u001b[39m=\u001b[39m [TRANSFORMS[t](\u001b[39m*\u001b[39mparse_args(args)[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparse_args(args)[\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m t, args \u001b[39min\u001b[39;00m transforms\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m     82\u001b[0m \u001b[39mreturn\u001b[39;00m albumentations\u001b[39m.\u001b[39mCompose(transforms)\n",
      "File \u001b[0;32m~/panoptic-lsb/data/training_utils.py:81\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m         kwargs\u001b[39m.\u001b[39mupdate(args)\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m pos_args, kwargs\n\u001b[0;32m---> 81\u001b[0m transforms \u001b[39m=\u001b[39m [TRANSFORMS[t](\u001b[39m*\u001b[39;49mparse_args(args)[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparse_args(args)[\u001b[39m1\u001b[39;49m]) \u001b[39mfor\u001b[39;00m t, args \u001b[39min\u001b[39;00m transforms\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m     82\u001b[0m \u001b[39mreturn\u001b[39;00m albumentations\u001b[39m.\u001b[39mCompose(transforms)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'limit'"
     ]
    }
   ],
   "source": [
    "from data.training_utils import lsb_datasets, construct_dataset\n",
    "from data import class_maps\n",
    "from panoptic.mrcnnhelper.utils import collate_fn\n",
    "import torch\n",
    "\n",
    "class_map = \"basichalosnocompanions\"\n",
    "\n",
    "def get_datasets(class_map):\n",
    "    # verify that class_map can be non-optional\n",
    "    if class_map in class_maps.class_maps:\n",
    "        return lsb_datasets(class_map)\n",
    "    else:\n",
    "        pass\n",
    "        # make sure classes is set\n",
    "        # return coco\n",
    "\n",
    "def get_data_loaders(dataset_train, dataset_val, dataset_test, batch_size=2):\n",
    "    # define training and validation data loaders\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=batch_size, shuffle=True, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    return data_loader_train, data_loader_val, data_loader_test\n",
    "\n",
    "# dataset_val exists because i'm lazy\n",
    "dataset_train, dataset_val, dataset_test = get_datasets(class_map)\n",
    "\n",
    "data_loader_train, _, data_loader_test = get_data_loaders(\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    dataset_test,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "# next(iter(data_loader_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panoptic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
